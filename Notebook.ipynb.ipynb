{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSUScgmSNBPD"
      },
      "source": [
        "#Replication package for \"Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tth_ojonFArh"
      },
      "source": [
        "## Instructions\n",
        "This Python notebook facilitates the replication of the findings presented in the manuscript. Please follow the prescribed order in which the scripts are presented. Interactive prompts will request any files necessary for fine-tuning the model, create the RAG-assisted models, testing it, and preparing the evaluations for the human reviewers.\n",
        "\n",
        "## Outputs\n",
        "All script outputs will be automatically downloaded to your browser. Specifically, the full responses generated by the all the models will be automatically downloaded.\n",
        "\n",
        "## Prerequisites\n",
        "- OpenAPI Key saved as a secret with the name \"openaikey\" in your environment (located on the left side of this notebook).\n",
        "- Installation of the openai library (version >= 7.2.0) in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6osVcfEHsl4"
      },
      "source": [
        "#Auxilliary functions and setup\n",
        "In this section, we provide all the auxilliary function to replicate our study and the constant values the other functions expect to find in the global namespace.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lw2XjEBLbMT"
      },
      "source": [
        "##Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DnJVVNTFAEC"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgTIMkZsytje"
      },
      "source": [
        "##Imports and Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7Ra2twxyuIA"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "from google.colab import files\n",
        "import json\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "import os\n",
        "import openai\n",
        "import json\n",
        "import re\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import pdb;\n",
        "\n",
        "\n",
        "file_encoding = 'utf-8-sig'\n",
        "csv_delimiter =\";\"\n",
        "csv_header = [\n",
        "    \"ID Scenario\",\n",
        "    \"Scenario Description\",\n",
        "    \"Short\",\n",
        "    \"Extended\",\n",
        "    \"RiskID\",\n",
        "    \"RiskDesc\",\n",
        "    \"VulnID\",\n",
        "    \"VulnDesc\",\n",
        "    \"RiskType\"\n",
        "]\n",
        "\n",
        "system_message='''You are an assistant in security risk analysis.\n",
        "      You need to determine if the current user message contains a security threat.\n",
        "      If a security threat is present, please explain what the security threat is.\n",
        "      You must reply with \\\"more\\\"  in the \\\"Short\\\" field if you think additional details should be provided along with the vulnerability already discovered\n",
        "      You must reply with \\\"no\\\"  in the \\\"Short\\\" field  if you think NO vulnerabilities are present\n",
        "      You must reply with \\\"yes\\\"  in the \\\"Short\\\" field  if you think there is at least one vulnerability\n",
        "      You must NEVER HALLUCINATE\n",
        "      You always have to reply in ITALIAN.\n",
        "      Always respond with an array of valid JSON output, for each vulnerability you find, create an item as the following and put into an array of json:\n",
        "      {\n",
        "      \\\"Extended\\\": \\\"[Extended description]\\\",\n",
        "      \\\"Short\\\":\t \\\"[Vulnerability Present: YES/NO/MORE]\\\",\n",
        "      \\\"Details\\\": \\\"[Vulnerability Description]\\\",\n",
        "      \\\"RiskID\\\":\t\\\"[Risk ID]\\\",\n",
        "      \\\"RiskDesc\\\": \\\"[Risk Description]\\\",\n",
        "      \\\"VulnID\\\":\t\\\"[Vulnerability ID]\\\",\n",
        "      \\\"VulnDesc\\\": \\\"[Vulnerability Description]\\\",\n",
        "      \\\"RiskType\\\": \\\"[Reale/Potenziale]\\\"\n",
        "      },'''\n",
        "\n",
        "system_message_rag='''You are an assistant in security risk analysis.\n",
        "      You need to determine if the current user message contains a security threat.\n",
        "      If a security threat is present, please explain what the security threat is.\n",
        "      You must reply with \\\"more\\\" if you think additional details should be provided along with the vulnerability already discovered\n",
        "      You must reply with \\\"no\\\"  if you think NO vulnerabilities are present\n",
        "      You must reply with \\\"yes\\\"  if you think there is at least one vulnerability\n",
        "      You must NEVER HALLUCINATE\n",
        "      You always have to reply in ITALIAN.\n",
        "      If you think \"yes\" or \"more\" You MUST list the identified vulnerability (vulnearbilit√†) and threat (minaccia) with the appropriate Identifiers refer to the document in your retrieval vectorstore\n",
        "      For the acronym  refer to the document in your retrieval vectorstore\n",
        "      Give all the information without asking the user more input\n",
        "      '''\n",
        "\n",
        "system_message_reformat='''You are an assistant in security risk analysis.\n",
        "      You need to format the user message as follows\n",
        "      If a security threat is present, please explain what the security threat is.\n",
        "      You must reply with \\\"more\\\"  in the \\\"Short\\\" field if you think additional details should be provided along with the vulnerability already discovered\n",
        "      You must reply with \\\"no\\\"  in the \\\"Short\\\" field  if you think NO vulnerabilities are present\n",
        "      You must reply with \\\"yes\\\"  in the \\\"Short\\\" field  if you think there is at least one vulnerability\n",
        "      You must NEVER HALLUCINATE\n",
        "      You always have to reply in ITALIAN.\n",
        "      If the message ONLY contain \"si\"/\"yes\" simply say in the short field \"yes\", and for the rest leave blank\n",
        "      If the message ONLY contain \"no\" simply say in the short field \"no\", and for the rest leave blank\n",
        "      Always respond with an array of valid JSON output, for each vulnerability/threat you find, create an item as the following and put into an array of json:\n",
        "      {\n",
        "      \\\"Extended\\\": \\\"[Extended description]\\\",\n",
        "      \\\"Short\\\":\t \\\"[Vulnerability Present: YES/NO/MORE]\\\",\n",
        "      \\\"Details\\\": \\\"[Vulnerability Description]\\\",\n",
        "      \\\"RiskID\\\":\t\\\"[Risk ID]\\\",\n",
        "      \\\"RiskDesc\\\": \\\"[Risk Description]\\\",\n",
        "      \\\"VulnID\\\":\t\\\"[Vulnerability ID]\\\",\n",
        "      \\\"VulnDesc\\\": \\\"[Vulnerability Description]\\\",\n",
        "      \\\"RiskType\\\": \\\"[Reale/Potenziale]\\\"\n",
        "      },'''\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openaikey')\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArplSTxSz6IM"
      },
      "source": [
        "##Data Management Handlers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laWKydyOM6sX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def csv_to_dict(csv_file):\n",
        "    \"\"\"\n",
        "    Converts a CSV file into a dictionary format.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the data from the CSV file.\n",
        "    \"\"\"\n",
        "    data_dict = {}\n",
        "    with open(csv_file, 'r',encoding=file_encoding) as file:\n",
        "        csv_reader = csv.DictReader(file, delimiter=csv_delimiter)\n",
        "        for row in csv_reader:\n",
        "            scenario_id = row['ID Scenario'].replace(\"'\",\"\")\n",
        "            user_message = row['User'].replace(\"'\",\"\")\n",
        "            result=row['Assistant - Short'].replace(\"'\",\"\")\n",
        "            if(result==\"No\"):\n",
        "              assistant_message = {\n",
        "                'Extended':row['Assistant - Extended'].replace(\"'\",\"\"),\n",
        "                'Short':row['Assistant - Short'].replace(\"'\",\"\"),\n",
        "              }\n",
        "            else:\n",
        "\n",
        "              assistant_message = {\n",
        "                  'Extended':row['Assistant - Extended'].replace(\"'\",\"\"),\n",
        "                  'Short':row['Assistant - Short'].replace(\"'\",\"\"),\n",
        "                  'Details': row['Assistant - Details'].replace(\"'\",\"\"),\n",
        "                  'RiskID': row['Assistant - Risk ID'].replace(\"'\",\"\"),\n",
        "                  'RiskDesc': row['Assistant - Risk description'].replace(\"'\",\"\"),\n",
        "                  'VulnID': row['Assistant - Vulnerability ID'].replace(\"'\",\"\"),\n",
        "                  'VulnDesc': row['Assistant - Vulnerability description'].replace(\"'\",\"\"),\n",
        "                  'RiskType': row['Assistant - Risk occurrence type'].replace(\"'\",\"\")\n",
        "              }\n",
        "\n",
        "            if scenario_id not in data_dict:\n",
        "                data_dict[scenario_id] = {'UserMessage': user_message, 'AssistantMessages': [], 'TotalAssistantMessages' : 0}\n",
        "            data_dict[scenario_id]['AssistantMessages'].append(assistant_message)\n",
        "    return data_dict\n",
        "\n",
        "\n",
        "\n",
        "def upload_file_and_get_path():\n",
        "    \"\"\"\n",
        "    Uploads a file and returns its path.\n",
        "\n",
        "    Returns:\n",
        "        str: The path of the uploaded file.\n",
        "    \"\"\"\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        return filename\n",
        "\n",
        "\n",
        "\n",
        "def convert_rawcsv_to_json_dict(csv_file):\n",
        "    \"\"\"\n",
        "    Converts a raw CSV file to a JSON dictionary.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): The path to the raw CSV file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A JSON dictionary containing the converted data.\n",
        "    \"\"\"\n",
        "    result = csv_to_dict(csv_file)\n",
        "    result = add_total_assistant_messages(result)\n",
        "    return result\n",
        "\n",
        "def store_json_file(destination, content):\n",
        "    \"\"\"\n",
        "    Stores JSON content to a file.\n",
        "\n",
        "    Args:\n",
        "        destination (str): The path to save the JSON file.\n",
        "        content (dict): The JSON content to be stored.\n",
        "    \"\"\"\n",
        "    with open(destination, 'w', encoding=file_encoding) as json_file:\n",
        "      json.dump(content, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "def load_json_file(source):\n",
        "    \"\"\"\n",
        "    Loads JSON content from a file.\n",
        "\n",
        "    Args:\n",
        "        source (str): The path of the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded JSON content.\n",
        "    \"\"\"\n",
        "    with open(source, 'r', encoding=file_encoding) as json_file:\n",
        "        data = json.load(json_file)\n",
        "    return data\n",
        "\n",
        "\n",
        "def max_assistant_messages(json_data):\n",
        "    \"\"\"\n",
        "    Finds the maximum number of assistant messages among scenarios.\n",
        "\n",
        "    Args:\n",
        "        json_data (dict): JSON data containing scenario information.\n",
        "\n",
        "    Returns:\n",
        "        int: The maximum number of assistant messages.\n",
        "    \"\"\"\n",
        "    max_messages = 0\n",
        "    for scenario_data in json_data.values():\n",
        "        assistant_messages_count = scenario_data['TotalAssistantMessages']\n",
        "        if assistant_messages_count > max_messages:\n",
        "            max_messages = assistant_messages_count\n",
        "    return max_messages\n",
        "\n",
        "\n",
        "def add_total_assistant_messages(json_data):\n",
        "    \"\"\"\n",
        "    Adds total assistant message count to each scenario in JSON data.\n",
        "\n",
        "    Args:\n",
        "        json_data (dict): JSON data containing scenario information.\n",
        "\n",
        "    Returns:\n",
        "        dict: JSON data with total assistant message counts added.\n",
        "    \"\"\"\n",
        "    for scenario_id, scenario_data in json_data.items():\n",
        "        total_messages = len(scenario_data['AssistantMessages'])\n",
        "        scenario_data['TotalAssistantMessages'] = total_messages\n",
        "    return json_data\n",
        "\n",
        "\n",
        "def create_questionaire(json_data):\n",
        "  \"\"\"\n",
        "  Creates a questionnaire CSV file based on JSON data.\n",
        "\n",
        "  Args:\n",
        "      json_data (dict): JSON data containing scenario information.\n",
        "  \"\"\"\n",
        "  # Create a list to hold the rows of the CSV\n",
        "  csv_rows = []\n",
        "\n",
        "  # Append header to the CSV rows\n",
        "  header = [\n",
        "    \"ID Scenario\",\n",
        "    \"Scenario Description\",\n",
        "    \"Result\",\n",
        "    \"Risk ID\",\n",
        "    \"Vulnerability ID\",\n",
        "    \"Risk occurrence type\"\n",
        "  ]\n",
        "\n",
        "  csv_rows.append(header)\n",
        "\n",
        "  # Iterate through the scenarios in the JSON data\n",
        "  for scenario_id, scenario_data in json_data.items():\n",
        "      # Extract Scenario ID and User Message\n",
        "      scenario_info = [scenario_id, scenario_data['UserMessage'],\"\",\"\",\"\",\"\"]\n",
        "      # Append the extracted info to the CSV rows\n",
        "      csv_rows.append(scenario_info)\n",
        "\n",
        "  # Write CSV\n",
        "  with open('questionaire.csv', 'w', newline='', encoding=file_encoding) as csv_file:\n",
        "      csv_writer = csv.writer(csv_file,delimiter=csv_delimiter)\n",
        "      csv_writer.writerows(csv_rows)\n",
        "\n",
        "  # Print message\n",
        "  print(\"questionaire CSV file has been created.\")\n",
        "\n",
        "\n",
        "\n",
        "def print_risk_type_set(json_data):\n",
        "    \"\"\"\n",
        "    Prints a set of unique risk types found in the JSON data.\n",
        "\n",
        "    Args:\n",
        "        json_data (dict): JSON data containing scenario information.\n",
        "    \"\"\"\n",
        "    # Insieme per memorizzare i Risk type unici\n",
        "    risk_types = set()\n",
        "\n",
        "    # Itera attraverso gli scenari per estrarre i Risk type unici\n",
        "    for scenario_data in json_data.values():\n",
        "        for assistant_message in scenario_data['AssistantMessages']:\n",
        "            if('RiskType' in assistant_message):\n",
        "              risk_types.add(assistant_message['RiskType'])\n",
        "\n",
        "    # Stampare l'insieme dei Risk type unici\n",
        "    print(\"Insieme dei Risk type unici:\")\n",
        "    for risk_type in risk_types:\n",
        "        print(risk_type)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "def write_string_to_file_and_download(string_data, filename):\n",
        "    \"\"\"\n",
        "    Writes a string to a file and downloads it.\n",
        "\n",
        "    Args:\n",
        "        string_data (str): The string data to be written to the file.\n",
        "        filename (str): The name of the file.\n",
        "\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding=file_encoding) as file:\n",
        "        file.write(string_data)\n",
        "    files.download(filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def splitTrainingAndValidation(jsonl_data):\n",
        "  \"\"\"\n",
        "    Splits JSONL data into training and validation sets and writes them to files.\n",
        "\n",
        "    Args:\n",
        "        jsonl_data (dict): JSONL data containing scenario information.\n",
        "  \"\"\"\n",
        "  # Calculate the number of entries for test and validation\n",
        "  total_entries = len(jsonl_data)\n",
        "  test_entries = int(0.7 * total_entries)\n",
        "  validation_entries = total_entries - test_entries\n",
        "\n",
        "  # Slice the original dictionary to create test and validation dictionaries\n",
        "  test = dict(list(jsonl_data.items())[:test_entries])\n",
        "  validation = dict(list(jsonl_data.items())[test_entries:])\n",
        "\n",
        "  # Call the function getJSONL on test and validation dictionaries\n",
        "  training = getJSONL(test)\n",
        "  write_string_to_file_and_download(training,\"training.jsonl\")\n",
        "  validation = getJSONL(validation)\n",
        "  write_string_to_file_and_download(validation,\"validation.jsonl\")\n",
        "\n",
        "def dump_array_to_json_and_download(data_array, filename):\n",
        "    # Write JSON data to a file\n",
        "    store_json_file(filename,data_array)\n",
        "\n",
        "    # Provide download link\n",
        "    files.download(filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def json_to_csv(json_data, csv_header,model,isAssistant):\n",
        "    # Open a CSV file in write mode\n",
        "    with open(f'output_{model}.csv', 'w', newline='', encoding=file_encoding) as csvfile:\n",
        "        writer = csv.writer(csvfile,delimiter=csv_delimiter)\n",
        "\n",
        "        # Write the CSV header\n",
        "        writer.writerow(csv_header)\n",
        "        #pdb.set_trace()\n",
        "        # Iterate over each JSON entry\n",
        "        for key,value in json_data.items():\n",
        "            if key:\n",
        "                if(isAssistant):\n",
        "\n",
        "                    for item in value:\n",
        "                        row = []\n",
        "                        if(item.strip(\"\\n\")==testing_dict[key][\"UserMessage\"]):\n",
        "                          continue\n",
        "                        for header in csv_header:\n",
        "                            # Stripping double and single quotes from header\n",
        "                            stripped_header = header.strip('\"').strip(\"'\")\n",
        "                            if(stripped_header==\"ID Scenario\"):\n",
        "                              row.append(key)\n",
        "                            elif(stripped_header==\"Scenario Description\"):\n",
        "                              row.append(testing_dict[key][\"UserMessage\"])\n",
        "                            elif(stripped_header==\"Extended\"):\n",
        "                              row.append(item)\n",
        "                            else:\n",
        "                              row.append(\"\")\n",
        "                        # Write the row to the CSV file\n",
        "                        writer.writerow(row)\n",
        "                else:\n",
        "                  for item in value:\n",
        "                    # Extract values for each key in the CSV header\n",
        "                    row = [item.get(header.strip('\"').strip(\"'\"), '') for header in csv_header]\n",
        "\n",
        "                    # Write the row to the CSV file\n",
        "                    writer.writerow(row)\n",
        "            else:\n",
        "                # If the JSON entry is empty, write an empty row with only scenario ID and description\n",
        "                writer.writerow(['', ''])\n",
        "    files.download(f'output_{model}.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSkH7kwjz1j1"
      },
      "source": [
        "##Fine Tuning and Result Handlers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sm5PTGJ2z0bc"
      },
      "outputs": [],
      "source": [
        "def startFineTunedModelTestingWithDict(model, testing_dict,assistant):\n",
        "    results = {}\n",
        "    full_responses = []\n",
        "    errors = {}\n",
        "\n",
        "    number_of_scenarios= len(testing_dict)\n",
        "    current = 1\n",
        "    for key, value in testing_dict.items():\n",
        "        current,full_responses,results,errors = extracted(key,value,current,number_of_scenarios,assistant,full_responses,results,errors)\n",
        "    dump_array_to_json_and_download(results, f\"results_{model}.json\")\n",
        "    dump_array_to_json_and_download(full_responses, f\"full_responses_{model}.json\")\n",
        "\n",
        "\n",
        "def runLLM(model,userMessages,assistant):\n",
        "\n",
        "  if(assistant):\n",
        "\n",
        "   return create_thread_and_send_message(model,userMessages);\n",
        "\n",
        "  else:\n",
        "    stream = client.chat.completions.create(\n",
        "                  model=model,\n",
        "                  temperature=0.35,\n",
        "                  max_tokens=4096,\n",
        "                  top_p=0.75,\n",
        "                  frequency_penalty=0,\n",
        "                  presence_penalty=0,\n",
        "                  messages=[\n",
        "                      {\"role\": \"system\", \"content\": system_message},\n",
        "                      {\"role\": \"user\", \"content\": userMessages}],\n",
        "                  stream=True,\n",
        "              )\n",
        "\n",
        "    result_content =\"\"\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            result_content+=chunk.choices[0].delta.content\n",
        "    return result_content\n",
        "\n",
        "def extracted(key,value,current,number_of_scenarios,assistant,full_responses,results,errors):\n",
        "  # Do something with the current message (print in this case)\n",
        "      userMessages = value[\"UserMessage\"]\n",
        "      print(f\"Sending Scenario #{current}/{number_of_scenarios}: {key} - {userMessages}\")\n",
        "      current+=1\n",
        "\n",
        "      result_content = runLLM(model, userMessages,assistant)\n",
        "\n",
        "      if(assistant):\n",
        "\n",
        "          print(f\"Response: {result_content} \\n\\n\")\n",
        "          full_responses.append(result_content)\n",
        "          # Now, 'parsed_dict' contains the data as a Python dictionary\n",
        "          results[key]=result_content\n",
        "          return current,full_responses,results,errors\n",
        "      else:\n",
        "        # Extract only the content from the OpenAI response\n",
        "        result_content = result_content.replace(\"```json\\n\",\"\").replace(\"```\",\"\").replace(\"\\\\n\",\"\")\n",
        "\n",
        "\n",
        "        # Parse the JSON-like data using the custom decoder\n",
        "        try:\n",
        "          jsonArray = json.loads(result_content)\n",
        "          final_array = []\n",
        "          for item in jsonArray:\n",
        "            item[\"ID Scenario\"] = key\n",
        "            item[\"Scenario Description\"] = value[\"UserMessage\"]\n",
        "            final_array.append(item)\n",
        "\n",
        "          print(f\"Response: {final_array} \\n\\n\")\n",
        "          full_responses.append(final_array)\n",
        "\n",
        "          # Now, 'parsed_dict' contains the data as a Python dictionary\n",
        "          results[key]=final_array\n",
        "          return current,full_responses,results,errors\n",
        "        except Exception as e:\n",
        "          print(f\"There was an error with {key}... Let's Retry\")\n",
        "          errors[key] = result_content\n",
        "          return extracted(key,value,current,number_of_scenarios,assistant,full_responses,results,errors)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR2DmySc9VTM"
      },
      "source": [
        "### Test JSONL Format [Optional]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wABuTXhk7pJl"
      },
      "outputs": [],
      "source": [
        "def testJSONLFile(filename):\n",
        "  from collections import defaultdict\n",
        "    # Load the dataset\n",
        "  with open(filename, 'r', encoding=file_encoding) as f:\n",
        "      dataset = []\n",
        "      for line in f:\n",
        "        json_string = line.replace(\"\\\"\", '\\\\\"')  # Replace single quotes with double quotes\n",
        "        json_string = json_string.replace(\"'\", '\"')\n",
        "        print(json_string)\n",
        "        decoded_json = json.loads(json_string)  # Decode JSON string\n",
        "        dataset.append(decoded_json)  # Append decoded JSON to dataset\n",
        "\n",
        "  # Initial dataset stats\n",
        "  print(\"Num examples:\", len(dataset))\n",
        "  print(\"First example:\")\n",
        "  for message in dataset[0][\"messages\"]:\n",
        "      print(message)\n",
        "\n",
        "  # Format error checks\n",
        "  format_errors = defaultdict(int)\n",
        "\n",
        "  for ex in dataset:\n",
        "      if not isinstance(ex, dict):\n",
        "          format_errors[\"data_type\"] += 1\n",
        "          continue\n",
        "\n",
        "      messages = ex.get(\"messages\", None)\n",
        "      if not messages:\n",
        "          format_errors[\"missing_messages_list\"] += 1\n",
        "          continue\n",
        "\n",
        "      for message in messages:\n",
        "          if \"role\" not in message or \"content\" not in message:\n",
        "              format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "          if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "              format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "          if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "              format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "          content = message.get(\"content\", None)\n",
        "          function_call = message.get(\"function_call\", None)\n",
        "\n",
        "          if (not content and not function_call) or not isinstance(content, str):\n",
        "              format_errors[\"missing_content\"] += 1\n",
        "\n",
        "      if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "          format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "  if format_errors:\n",
        "      print(\"Found errors:\")\n",
        "      for k, v in format_errors.items():\n",
        "          print(f\"{k}: {v}\")\n",
        "      return False\n",
        "  else:\n",
        "      print(\"No errors found\")\n",
        "      return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bFojFPdEGYX"
      },
      "source": [
        "## Assistant API Auxilliary Functions\n",
        "[Documentation on Assistant-Knowledge Retrieval](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qimNYMT9EHzc"
      },
      "outputs": [],
      "source": [
        "def handleAssistantCreation(model,file_ids):\n",
        "  assistant = client.beta.assistants.create(\n",
        "    instructions=system_message_rag,\n",
        "    model=model,\n",
        "    tools=[{\"type\": \"retrieval\"}],\n",
        "    file_ids=file_ids\n",
        "  )\n",
        "  return assistant\n",
        "\n",
        "def addFileForRetrieval(filename):\n",
        "  # Upload a file with an \"assistants\" purpose\n",
        "  file = client.files.create(\n",
        "    file=open(filename, \"rb\"),\n",
        "    purpose='assistants'\n",
        "  )\n",
        "  return file\n",
        "\n",
        "def uploadAndGetList():\n",
        "  table_vuln = upload_file_and_get_path()\n",
        "  table_risk = upload_file_and_get_path()\n",
        "  glossary = upload_file_and_get_path()\n",
        "\n",
        "  table_vuln_id = addFileForRetrieval(table_vuln).id\n",
        "  table_risk_id = addFileForRetrieval(table_risk).id\n",
        "  glossary_id = addFileForRetrieval(glossary).id\n",
        "  file_ids=[table_vuln_id,table_risk_id,glossary_id]\n",
        "  return file_ids\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def create_thread_and_send_message(assistant, message_content):\n",
        "  thread = client.beta.threads.create()\n",
        "  message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=message_content\n",
        "  )\n",
        "  run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant,\n",
        "    instructions=system_message_rag\n",
        "  )\n",
        "\n",
        "  while run.status in ['queued', 'in_progress', 'cancelling']:\n",
        "    time.sleep(1) # Wait for 1 second\n",
        "    run = client.beta.threads.runs.retrieve(\n",
        "      thread_id=thread.id,\n",
        "      run_id=run.id\n",
        "    )\n",
        "\n",
        "  if run.status == 'completed':\n",
        "    retrieved_message = client.beta.threads.messages.list(\n",
        "      thread_id=thread.id\n",
        "    )\n",
        "    print(retrieved_message.data[0].content[0].text.value)\n",
        "\n",
        "    results= []\n",
        "    for assistant_message in retrieved_message.data:\n",
        "      #results.append(assistant_message.content[0].text.value)\n",
        "      results.append(extract_message_and_annotations(assistant_message))\n",
        "    return results\n",
        "  else:\n",
        "    print(\"ERRORE!\")\n",
        "    print(run.status)\n",
        "\n",
        "  client.beta.threads.delete(thread.id)\n",
        "\n",
        "def extract_message_and_annotations(message):\n",
        "  # Extract the message content\n",
        "  message_content = message.content[0].text\n",
        "  annotations = message_content.annotations\n",
        "  citations = []\n",
        "\n",
        "  # Iterate over the annotations and add footnotes\n",
        "  for index, annotation in enumerate(annotations):\n",
        "      # Replace the text with a footnote\n",
        "      message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
        "\n",
        "      # Gather citations based on annotation attributes\n",
        "      if (file_citation := getattr(annotation, 'file_citation', None)):\n",
        "          cited_file = client.files.retrieve(file_citation.file_id)\n",
        "          citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
        "      elif (file_path := getattr(annotation, 'file_path', None)):\n",
        "          cited_file = client.files.retrieve(file_path.file_id)\n",
        "          citations.append(f'[{index}] Click <here> to download {cited_file.filename}')\n",
        "          # Note: File download functionality not implemented above for brevity\n",
        "    # Add footnotes to the end of the message before displaying to user\n",
        "  message_content.value += '\\n' + '\\n'.join(citations)\n",
        "  return message_content.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXAfBKNGH6fR"
      },
      "source": [
        "##Training and Testing set creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Y8Mwfg0bKT"
      },
      "source": [
        "The following methods requires two csv files containing the training and the testing scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVV7UfzUOoA6"
      },
      "outputs": [],
      "source": [
        "training_raw = upload_file_and_get_path()\n",
        "testing_raw = upload_file_and_get_path()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYLeiQ5j0mfJ"
      },
      "source": [
        "We translate the raw scenarios into dictionaries with the Scenario ID as key and the assistant message fully assembled from its raw csv version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqpiv7w0OPcK"
      },
      "outputs": [],
      "source": [
        "training_dict = convert_rawcsv_to_json_dict(training_raw)\n",
        "testing_dict = convert_rawcsv_to_json_dict(testing_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY-CwjGi00UT"
      },
      "source": [
        "We save locally the json files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M2QNQhVSa_L"
      },
      "outputs": [],
      "source": [
        "store_json_file(\"training_dict.json\", training_dict)\n",
        "store_json_file(\"testing_dict.json\", testing_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MM98cQo03a0"
      },
      "source": [
        "We translate the training dictionary into JSONL format for fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUXK_LLMIutZ"
      },
      "source": [
        "##Fine-Tuning Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQGKth3M1Sbg"
      },
      "source": [
        "We split the training data into training and validation sets with 70-30 ratio. We use the online GUI to perform the fine-tuning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOsJd7x99awd"
      },
      "outputs": [],
      "source": [
        "def getJSONL(json_data):\n",
        "    \"\"\"\n",
        "    Converts JSON data into a format suitable for fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        json_data (dict): JSON data containing scenario information.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the JSON-Lines formatted data.\n",
        "    \"\"\"\n",
        "    jsonl = []\n",
        "    for scenario_data in json_data.values():\n",
        "        user_message = scenario_data['UserMessage'].replace(\"'\",\"\")#.replace(\"'\", \"\\'\")\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "            {\"role\": \"assistant\", \"content\": json.dumps(scenario_data['AssistantMessages'],ensure_ascii=False)}]\n",
        "        item_to_append = {\"messages\":messages}\n",
        "\n",
        "        #print(item_to_append)\n",
        "        jsonl.append(item_to_append)\n",
        "    ret = \"\"\n",
        "    for line in jsonl:\n",
        "      jsonl_line= json.dumps(line,ensure_ascii=False).replace(\"'\", '\"')\n",
        "      ret += f\"{jsonl_line}\\n\"\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "sZ8hmeRoZumd",
        "outputId": "3cbc0574-a550-4aa2-d7ef-5f49ebb79874"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_1916f1bb-7676-444e-9ca4-32c210433141\", \"training.jsonl\", 535893)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rest = getJSONL(training_dict)\n",
        "write_string_to_file_and_download(rest,\"training.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "K_mRA8nD4BWI",
        "outputId": "a309498c-76da-4a2d-c418-ee2bbe1ff2c3"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_10334c84-a9d9-4c8e-a716-4ab6fefd8c53\", \"training.jsonl\", 386541)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_704e4ac9-2b3e-4708-846c-a7c6c1f3d7a5\", \"validation.jsonl\", 149355)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "splitTrainingAndValidation(training_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWqXcSNeIBY4"
      },
      "source": [
        "#Questionaire Creation\n",
        "The following creates, and download, a CSV file containing the scenarios for  human experts to analyze. Those scenarios are the same for human experts and LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBPPAY56WnYj"
      },
      "outputs": [],
      "source": [
        "create_questionaire(testing_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJhab9EZ1jTN"
      },
      "source": [
        "#Model Testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF9VQ6m3JSWs"
      },
      "source": [
        "Below, you can initiate testing the fine-tuned model on the provided data. Please use the appropriate Python methods to generate the assistants' IDs. Similarly, use OpenAI's GUI to fine-tune the model and generate the ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKRq0YDTJxMU"
      },
      "outputs": [],
      "source": [
        "base = \"gpt-3.5-turbo-1106\"\n",
        "base_fine_tuned = \"xxxxxxxxxx\"\n",
        "base_four_turbo = \"gpt-4-turbo-preview\"\n",
        "assistant_base = \"xxxxxxxxxx\"\n",
        "asssistant_base_four = \"xxxxxxxxxx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0EYg35IL5zU"
      },
      "outputs": [],
      "source": [
        "file_ids = uploadAndGetList()\n",
        "assistant_base = handleAssistantCreation(base,file_ids).id\n",
        "asssistant_base_four = handleAssistantCreation(base_four_turbo,file_ids).id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gETjIjDeLrxT"
      },
      "outputs": [],
      "source": [
        "model = asssistant_base_four\n",
        "isAssistant=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-J3_ylOzFBo"
      },
      "outputs": [],
      "source": [
        "startFineTunedModelTestingWithDict(model, testing_dict,isAssistant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvMx1RB11qJr"
      },
      "source": [
        "#Output Preparation for Human Reviewers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKJSEAkuxlwP"
      },
      "outputs": [],
      "source": [
        "test_result = upload_file_and_get_path()\n",
        "json_data = load_json_file(test_result)\n",
        "json_to_csv(json_data, csv_header,model,isAssistant)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0lw2XjEBLbMT",
        "AWqXcSNeIBY4",
        "c76Otso07cZV"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
